{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load episode set $E$ with the users that retweeted each original tweet in the trace. \n",
    "\n",
    "Each episode $E_{s}$ includes the users that retweeted s, ordered chronologically, as they appear in the trace. The first user in each episode is the user that originally tweeted the tweet, and is denoted by $r_{s}$. Subsequent users in $E_{s}$ are users that retweeted s, either directly from user $r_{s}$ or from another user that appears in Es before them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = pickle.load(open(\"./extracted/E\"+ str(lines) + \".p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in E:\n",
    "    E[s] = list(dict.fromkeys(E[s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the set of original tweets denoted by $S$. \n",
    "\n",
    "The set of original tweets is denoted by $S$, where |$S$| = S is the total number of original tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pickle.load(open(\"./extracted/S\"+ str(lines) + \".p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load $U$ set with unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = pickle.load(open(\"./extracted/U\"+ str(lines) + \".p\", \"rb\"))\n",
    "U = list(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load $D$ dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pickle.load(open(\"./extracted/D\"+ str(lines) + \".p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Find important quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(U)\n",
    "print('Number of unique users N =', N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Episodes (original tweets) S =',len(E))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saito's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(obj):\n",
    "    if type(obj) == list:\n",
    "        return [l for L in obj for l in L]\n",
    "    if type(obj) == dict:\n",
    "        return [l for i in obj for l in obj[i].values()]\n",
    "    if type(obj) == defaultdict:\n",
    "        return [l for i in obj for l in obj[i].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saito(eps, D):\n",
    "        \"\"\" \n",
    "        This function is the main algorithm for path inference with constraints according to Saito et al. [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            eps : float\n",
    "                Convergence criterion.\n",
    "            D : dict\n",
    "                Dictionary with D_{ij} values that was created in the trace-preprocessing.ipynb notebook.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "            k : dict\n",
    "                Dictionary that includes the influence probabilities k_{ij} for each (i,j) pair\n",
    "\n",
    "\n",
    "        [1] K. Saito, R. Nakano,  and M. Kimura, ``Prediction of Information Diffusion Probabilities \n",
    "        for Independent Cascade Model'', in International Conference on Knowledge-Based and Intelligent \n",
    "        Information and Engineering Systems}, vol. 5179, 2008, pp. 67-75.\n",
    "        \"\"\"\n",
    "\n",
    "        iterat = 1\n",
    "        # ======================== INITIALIZE ========================\n",
    "\n",
    "        k = defaultdict(dict)\n",
    "        Splus = defaultdict(dict)\n",
    "        Sminus = defaultdict(dict)\n",
    "        for s in D:\n",
    "                for t in list(D[s]):\n",
    "                        if t!=0:\n",
    "                            for w in D[s][t]:\n",
    "                                indx = list(D[s]).index(t)\n",
    "                                previous_t = list(D[s])[indx-1:indx]\n",
    "                                for p_t in previous_t:\n",
    "                                    for u in list(D[s][p_t]):\n",
    "                                        k[u][w] = random.uniform(0,1)\n",
    "                                        if u in Splus and w in Splus[u]:\n",
    "                                            Splus[u][w].append(s)\n",
    "                                        else:\n",
    "                                            Splus[u][w]=[s]\n",
    "        \n",
    "        for s in D:\n",
    "            for u in flatten(list(D[s].values())):\n",
    "                for w in k[u]:\n",
    "                    if s not in Splus[u][w]:\n",
    "                        if u in Sminus and w in Sminus[u]:\n",
    "                            Sminus[u][w].append(s)\n",
    "                        else:\n",
    "                            Sminus[u][w]=[s]                        \n",
    "        \n",
    "        for u in k:\n",
    "            for w in k[u]:\n",
    "                if w not in Sminus[u]:\n",
    "                    Sminus[u][w] = [] \n",
    "            \n",
    "\n",
    "        # ======================== START ========================\n",
    "\n",
    "        while True:\n",
    "            # Step 1 ==== UPDATE VALUES ====\n",
    "            P = defaultdict(dict)\n",
    "            for s in D:\n",
    "                    for t in list(D[s]):\n",
    "                            if t!=0:\n",
    "                                for w in D[s][t]:\n",
    "                                    indx = list(D[s]).index(t)\n",
    "                                    previous_t = list(D[s])[indx-1:indx]\n",
    "                                    pr = 1 \n",
    "                                    for p_t in previous_t:\n",
    "                                        for u in list(D[s][p_t]):\n",
    "                                            pr*=(1-k[u][w])\n",
    "                                    P[w][s] = 1 - pr \n",
    "            for u in k:\n",
    "                for w in k[u]:\n",
    "                    k[u][w] = (1/(len(Splus[u][w])+ len(Sminus[u][w]))) *sum(k[u][w]/P[w][s] for s in Splus[u][w])\n",
    "            \n",
    "            if iterat > 1:\n",
    "                new_k = np.array(flatten(k))\n",
    "                new_P = np.array(flatten(P))\n",
    "                changek = np.linalg.norm(new_k - old_k)                  \n",
    "                changeP = np.linalg.norm(new_P - old_P)  \n",
    "                if changek < eps and changeP < eps: \n",
    "                    old_k = np.array(flatten(k))\n",
    "                    old_P = np.array(flatten(P))\n",
    "                    break\n",
    "                else: \n",
    "                    old_k = np.array(flatten(k))\n",
    "                    old_P = np.array(flatten(P))\n",
    "                    iterat += 1\n",
    "            if iterat == 1:\n",
    "                flag = False\n",
    "                old_k = np.array(flatten(k))\n",
    "                old_P = np.array(flatten(P))\n",
    "                iterat+=1\n",
    "        # ======================== END ========================\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "eps = 10**-3\n",
    "k = saito(eps, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(k, open(\"./extracted/k_saito_\"+ str(lines) + \".p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
