{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate here how many lines to read from the trace\n",
    "lines = 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Import file path\n",
    "\n",
    "Base path: datasets folder\n",
    "- Russian: \"russian_rtid.txt\" (twid ts uid rtid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../datasets/\"\n",
    "filename = \"russian_rtid.txt\"\n",
    "path_to_file = os.path.join(base_path, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Add headers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = [\"twid\", \"ts\", \"uid\", \"rtid\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Read file with headers and print its first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "initial_file = pd.read_csv(path_to_file, sep=\"\\s+\", names=header_list)\n",
    "\n",
    "# sorting by time\n",
    "initial_file.sort_values(\"ts\", inplace = True)\n",
    "\n",
    "# dropping ALL duplicate values \n",
    "initial_file.drop_duplicates(subset =\"twid\", \n",
    "                     keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 the data that we are going to work with is the 'input_file' dataframe containining the number of lines we indicated in line 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = initial_file[0:lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Filter data:\n",
    "- We want an original twid to be retweeted from at least one user \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = input_file[((input_file['twid'].isin(input_file['rtid']))) & ((input_file['rtid']==-1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want all rtid (!=1) to exist in the twid column for completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = input_file[((input_file['rtid'].isin(input_file['twid']))) & ((input_file['rtid']!=-1)) & ((input_file['rtid']!=974014568073695232))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merge both sets of data into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data1, data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our final data includes', len(data), 'lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Save original tweets in the whole trace\n",
    "Original tweets are the tweets with rtid = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_ids = list(set(list(data['twid'].loc[data['rtid'] == -1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Create set of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = data.uid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique users in the trace:', len(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    pickle.dump(U, open(\"./extracted/U\" + str(lines) + \".p\", \"wb\"))\n",
    "except: \n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create episodes\n",
    "\n",
    "- For each original tweet t that we detect in $P$, we construct the set of its retweets, which we call episode and denote by $E_{s}$.\n",
    "- The whole set of episodes is denoted by $E$ and includes o_ids episodes in total, which is the number of original tweets/\n",
    "- We count $M_{ij}$ out of the o_ids total episodes where $(i,j)$ appears as an ordered pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create tweets dictionaries\n",
    "\n",
    "Dictionary E includes the u_ids of the users that retweeted each tweet s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by rtid\n",
    "gb = data.groupby(['rtid'])\n",
    "E = gb['uid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to default dict\n",
    "E = E.to_dict(defaultdict(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tweets with rtid=-1\n",
    "del(E[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create $s_t$ dictionary with the original tweets\n",
    "\n",
    "Each source $r_{s}$ will be included in dictionary S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = data.loc[data['rtid']==-1][['twid', 'uid']]\n",
    "S = S.set_index('twid').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save S dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    pickle.dump(s_t, open(\"./extracted/S\"+ str(lines) + \".p\", \"wb\"))\n",
    "except: \n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the user who originally tweeted each tweet in the first position of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in E:\n",
    "    E[ep] = np.insert(E[ep], 0, S[ep], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the duplicates from each tweet and remove tweets with no retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in list(E):\n",
    "    E[ep] = list(dict.fromkeys(E[ep]))   \n",
    "    if len(E[ep])==1:\n",
    "        E.pop(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    pickle.dump(E, open(\"./extracted/E\"+ str(lines) + \".p\", \"wb\"))\n",
    "except: \n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a $D$ dictionary, which contains the users that retweeted each tweet, in each timestamp:\n",
    "- key1: episode $s$\n",
    "- key2: timestamps $ts$ of episode $s$\n",
    "- values: list of users that retweeted $s$ at $ts$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = dict()\n",
    "for s in E:\n",
    "    D[s] = dict()\n",
    "    D[s][0] = [list(data['uid'].loc[data['twid']==s])[0]]\n",
    "\n",
    "for s in E:\n",
    "    visited_users = []\n",
    "    retweets = data[['uid','ts']].loc[data['rtid']==s]\n",
    "    for index, row in retweets.iterrows():\n",
    "        if row['uid']!=D[s][0][0] and row['uid'] not in visited_users: # remove cases where user retweets again  and # remove cases where user retweets himself\n",
    "            visited_users.append(row['uid'])\n",
    "            if row['ts'] in D[s]: \n",
    "                D[s][row['ts']].append(row['uid'])\n",
    "            else:\n",
    "                D[s][row['ts']] = []\n",
    "                D[s][row['ts']].append(row['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    pickle.dump(D, open(\"./extracted/D\"+ str(lines) + \".p\", \"wb\"))\n",
    "except: \n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Count $M_{ij}$ out of the o_ids total episodes where $(i,j)$ appears as an ordered pair according to D. (difference that users at the same timestamp do not count as an ordered pair) \n",
    "\n",
    "Create dictionary d_f that has for key a user and for values the users that appear after him and then create a $M$ dictionary with the total times a $(i,j)$ pair appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict()\n",
    "for s in D:\n",
    "    for users_list in list(D[s].values()):\n",
    "        index_now = list(D[s].values()).index(users_list)\n",
    "        for u in users_list:\n",
    "            u_after = list(D[s].values())[index_now+1:]\n",
    "            if u in d:\n",
    "                if len(u_after)!=0:\n",
    "                    d[u].append(u_after)\n",
    "            else:\n",
    "                if len(u_after)!=0:\n",
    "                    d[u] = [u_after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_f = defaultdict()\n",
    "for i in d: \n",
    "    d_f[i] = [user for sublist in d[i] for item in sublist for user in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = defaultdict()\n",
    "for i in d_f:\n",
    "    M[i] = Counter(d_f[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    pickle.dump(M, open(\"./extracted/M\" + str(lines) + \".p\", \"wb\"))\n",
    "except: \n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create E_newman to evaluate and compare our method with Newman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_retweet = defaultdict(list)\n",
    "total_retweets = 0 \n",
    "for index, tweet in data.iterrows():\n",
    "    user_id = tweet['uid']\n",
    "    tweet_id = tweet['twid']\n",
    "    retweet_id = tweet['rtid']\n",
    "    if retweet_id!=-1:\n",
    "        try:\n",
    "            retweeted_by_id = int(input_file['uid'].loc[input_file['twid']==retweet_id])\n",
    "            total_retweets += 1\n",
    "            user_id_to_retweet[user_id].append(retweeted_by_id)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eij_newman = defaultdict(dict)\n",
    "for user in user_id_to_retweet:\n",
    "        for retweet in user_id_to_retweet[user]:\n",
    "            if not retweet in Eij_newman[user]:\n",
    "                Eij_newman[user][retweet] = 1\n",
    "            else:\n",
    "                Eij_newman[user][retweet] += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    pickle.dump(Eij_newman, open(\"./extracted/E_newman\"+ str(lines) + \".p\", \"wb\"))\n",
    "except: \n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
